# -*- coding: utf-8 -*-
"""QLoRA_FineTuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17VwnRTusN9wFcT7XqtxTF0wkJehYcSnw
"""

INSTALL_PHASE = True

import time

# from IPython.display import clear_output
#
# if INSTALL_PHASE:
#   !rm -rf /kaggle/working/results
#   !rm -rf /opt/conda/lib/python3.10/site-packages/aiohttp*
#
#   !touch /opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1dist-info/METADATA
#
#   !pip install aiohttp>=3.9.2
#   !pip install --upgrade aiobotocore
#   !pip install accelerate
#   !pip install -i https://pypi.org/simple/ bitsandbytes
#
# !pip install accelerate
#
# if INSTALL_PHASE:
#   !conda install pytorch=2.3 pytorch-cuda=12.1 -c pytorch -c nvidia -y
#
#   clear_output()
#
# if INSTALL_PHASE:
#   !rm -rf /kaggle/working/results
#   !rm -rf /opt/conda/lib/python3.10/site-packages/aiohttp*
#   !touch /opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA
#   !pip install bitsandbytes
#   !pip install peft
#   !pip install trl

import os
os.environ["HF_TOKEN"] = ""
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ['TRANSFORMERS_CACHE'] = '/kaggle/working'

MODEL_NAME = "CohereForAI/aya-23-8b"

# 다음 매개변수는 GPU 구성에 따라 변경해야 할 수 있습니다.

QUANTIZE_4BIT = True  # 모델을 4비트로 양자화(Quantize)할지 여부를 설정합니다. 양자화는 모델의 메모리 사용량을 줄이고, 일부 경우에 성능을 향상시킬 수 있습니다.
USE_GRAD_CHECKPOINTING = True  # 그래디언트 체크포인팅을 사용할지 여부를 설정합니다. 이는 메모리 사용량을 줄여주지만, 훈련 속도는 느려질 수 있습니다.
TRAIN_BATCH_SIZE = 8  # 훈련 시 사용되는 배치 크기를 설정합니다. 이 값은 사용 가능한 GPU 메모리에 따라 조정될 수 있습니다.
TRAIN_MAX_SEQ_LENGTH = 512  # 훈련 시 사용되는 최대 시퀀스 길이를 설정합니다. 시퀀스 길이가 길어질수록 메모리 사용량이 증가합니다.
GRAD_ACC_STEPS = 4  # 그래디언트 누적 단계 수를 설정합니다. 이는 작은 배치 크기로 훈련할 때 유용하며, 효과적으로 배치 크기를 늘려줍니다.

attn_implementation = None  # 주의 메커니즘(Attention Mechanism) 구현을 위한 변수를 초기화합니다. 필요에 따라 적절한 구현을 설정할 수 있습니다.

# !pip uninstall -y flash-attn
# !pip install flash-attn
# !pip install --upgrade bitsandbytes
# !pip install accelerate
# !pip install aiohttp>=3.9.2
# !pip install --upgrade aiobotocore

import torch  # PyTorch 라이브러리를 불러옵니다. PyTorch는 딥러닝 모델을 구축하고 훈련하는 데 사용되는 주요 라이브러리입니다.
import accelerate as acc  # Hugging Face의 Accelerate 라이브러리를 불러옵니다. 이 라이브러리는 다중 GPU 및 분산 훈련을 쉽게 설정할 수 있도록 도와줍니다.
import bitsandbytes as bnb  # bitsandbytes 라이브러리를 불러옵니다. 이 라이브러리는 양자화된 연산을 지원하여 메모리 사용량을 줄이고 훈련 속도를 높일 수 있습니다.

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
# transformers 라이브러리에서 다양한 모듈을 불러옵니다.
# AutoModelForCausalLM: 사전 학습된 언어 모델을 로드하기 위한 클래스
# AutoTokenizer: 사전 학습된 토크나이저를 로드하기 위한 클래스
# BitsAndBytesConfig: 양자화된 모델 설정을 위한 클래스
# TrainingArguments: 모델 훈련 설정을 위한 클래스

from peft import LoraConfig  # PEFT(Pytorch Efficient Fine-Tuning) 라이브러리에서 LoraConfig 클래스를 불러옵니다. 이 클래스는 LoRA(Low-Rank Adaptation) 설정을 관리합니다.
import os  # 운영 체제 관련 기능을 제공하는 표준 라이브러리를 불러옵니다.
import torch  # 중복된 import 문. 이미 위에서 불러왔기 때문에 필요하지 않습니다.

from datasets import load_dataset  # Hugging Face Datasets 라이브러리에서 load_dataset 함수를 불러옵니다. 이 함수는 다양한 형식의 데이터셋을 쉽게 로드할 수 있게 합니다.
from trl import SFTTrainer  # TRL(Transformers Reinforcement Learning) 라이브러리에서 SFTTrainer 클래스를 불러옵니다. 이 클래스는 SFT(Supervised Fine-Tuning)를 위한 트레이너를 제공합니다.
from datasets import Dataset  # Hugging Face Datasets 라이브러리에서 Dataset 클래스를 불러옵니다. 이 클래스는 데이터셋을 다루기 위한 주요 객체입니다.

quantization_config = None  # 양자화 설정을 위한 변수를 초기화합니다.
if QUANTIZE_4BIT:  # QUANTIZE_4BIT 플래그가 True일 경우 양자화 설정을 구성합니다.
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 모델을 4비트로 양자화하여 로드합니다.
        bnb_4bit_quant_type="nf4",  # 4비트 양자화 유형을 'nf4'로 설정합니다.
        bnb_4bit_use_double_quant=True,  # 이중 양자화를 사용합니다.
        bnb_4bit_compute_dtype=torch.bfloat16,  # 계산 시 사용할 데이터 타입을 bfloat16으로 설정합니다.
    )

# 사전 학습된 언어 모델을 로드합니다.
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,  # 사전 학습된 모델의 이름을 지정합니다.
    quantization_config=quantization_config,  # 양자화 설정을 적용합니다 (양자화가 활성화된 경우).
    attn_implementation=attn_implementation,  # 주의 메커니즘(Attention Mechanism) 구현 설정을 적용합니다.
    torch_dtype=torch.bfloat16,  # 모델의 데이터 타입을 bfloat16으로 설정합니다.
    device_map="auto",  # 사용 가능한 모든 GPU를 자동으로 매핑하여 분산 훈련을 설정합니다.
)

print(model)

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def get_message_format(prompts):
    messages = []  # 메시지를 저장할 빈 리스트를 초기화합니다.

    for p in prompts:  # 주어진 모든 프롬프트에 대해 반복합니다.
        messages.append(
        [{"role": "user", "content": p}]  # 각 프롬프트를 딕셔너리 형식으로 변환하여 리스트에 추가합니다.
      )

    return messages  # 변환된 메시지 리스트를 반환합니다.

# 테스트용 프롬프트 생성 (Aya 23 세트의 언어로 된 프롬프트)
prompts = [
    "Write a list of three fruits and tell me about each of them",  # 영어 프롬프트
    "Viết danh sách ba loại trái cây và kể cho tôi nghe về từng loại trái cây đó",  # 베트남어 프롬프트
    "3 つの果物のリストを書いて、それぞれについて教えてください",  # 일본어 프롬프트
    "과일 리스트 3개를 알려줘, 그리고 각각에 대한 설명 부탁해",  # 한국어 프롬프트
]

get_message_format(prompts)  # 주어진 프롬프트를 메시지 형식으로 변환합니다.

def generate_aya_23(
      prompts,
      model,
      temperature=0.3,
      top_p=0.75,
      top_k=0,
      max_new_tokens=1024
    ):
    """
    주어진 프롬프트를 사용하여 모델을 통해 텍스트를 생성하는 함수입니다.

    인자:
    - prompts: 텍스트 프롬프트의 리스트
    - model: 사전 학습된 언어 모델
    - temperature: 생성의 다양성을 제어하는 파라미터 (0.3 기본값)
    - top_p: 상위 p% 확률의 토큰만 고려하는 파라미터 (0.75 기본값)
    - top_k: 상위 k개의 토큰만 고려하는 파라미터 (0 기본값)
    - max_new_tokens: 생성할 최대 토큰 수 (1024 기본값)
    """

    # 프롬프트를 메시지 형식으로 변환합니다.
    messages = get_message_format(prompts)

    # 메시지를 토크나이저를 사용하여 입력 ID로 변환합니다.
    input_ids = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=True,
        padding=True,
        return_tensors="pt",
      )

    # 입력 ID를 모델의 장치로 이동시킵니다.
    input_ids = input_ids.to(model.device)
    prompt_padded_len = len(input_ids[0])  # 패딩된 입력 ID의 길이를 저장합니다.

    # 모델을 사용하여 텍스트를 생성합니다.
    gen_tokens = model.generate(
        input_ids,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        max_new_tokens=max_new_tokens,
        do_sample=True,
      )

    # 생성된 토큰 중에서 입력 프롬프트 이후의 토큰만 가져옵니다.
    gen_tokens = [
      gt[prompt_padded_len:] for gt in gen_tokens
    ]

    # 생성된 토큰을 텍스트로 디코딩합니다.
    gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)
    return gen_text  # 생성된 텍스트를 반환합니다.

# Aya 23 세트의 언어로 텍스트 생성을 테스트합니다.
generations = generate_aya_23(prompts, model)  # 주어진 프롬프트와 모델을 사용하여 텍스트를 생성합니다.

# 프롬프트와 생성된 응답을 쌍으로 출력합니다.
for p, g in zip(prompts, generations):  # 각 프롬프트와 생성된 텍스트 쌍에 대해 반복합니다.
    print(
        "PROMPT", p, "RESPONSE", g, "\n", sep="\n"  # 프롬프트와 응답을 출력합니다. 각 요소는 새로운 줄에 출력됩니다.
    )

prompts = [
      'Translate from English to Korean: Rates are competitive, almost always the best in the market',
      'Translate from English to Korean: Two far-right Israeli ministers threaten to topple the government if it accepts Biden peace plan'
]

generations = generate_aya_23(prompts, model)

for p, g in zip(prompts, generations):
    print(
      "PROMPT", p ,"RESPONSE", g, "\n", sep="\n"
    )

import json  # JSON 형식의 파일을 읽고 쓰기 위해 사용되는 표준 라이브러리
import random  # 무작위 작업을 수행하기 위한 표준 라이브러리
from tqdm import tqdm  # 반복문 진행 상황을 시각적으로 표시해 주는 라이브러리

# JSON 파일 경로
json_path = '/content/EN_TO_KO.json'

# JSON 파일을 열고 텍스트 항목을 읽습니다.
with open(json_path, encoding='utf-8') as f:
    items = [json.loads(line)['text'] for line in tqdm(f)]  # 각 라인을 JSON으로 로드하고, 'text' 필드를 추출하여 리스트에 저장

# 영어와 한국어 문장을 분리하는 함수 정의
def split_eng_kor(text):
    # 텍스트를 줄바꿈 문자로 분리하여 소스와 타겟을 구분
    lines = text.split('\n')

    # 적절한 줄에서 소스와 타겟을 추출
    eng = lines[0].replace('### source: ', '').strip()  # 소스 텍스트에서 '### source: ' 제거하고 공백 제거
    kor = lines[2].replace('### target: ', '').strip()  # 타겟 텍스트에서 '### target: ' 제거하고 공백 제거
    return eng, kor  # 영어와 한국어 텍스트 반환

# 각 항목을 영어와 한국어 문장으로 분리
items = [split_eng_kor(e) for e in tqdm(items)]  # 각 항목에 대해 split_eng_kor 함수 적용

# 항목들을 무작위로 섞기
random.shuffle(items)

# 처음 20,000개의 항목을 입력과 타겟 리스트로 분리
inputs = [e for e, k in tqdm(items[:1000])]  # 처음 20,000개의 입력 텍스트 추출
targets = [k for e, k in tqdm(items[:1000])]  # 처음 20,000개의 타겟 텍스트 추출

# 입력과 타겟을 포함하는 데이터셋 생성
dataset = Dataset.from_dict({'inputs': inputs, 'targets': targets})

# 영어와 한국어 문장을 포맷팅하는 함수 정의
def convert_to_input_text(eng, kor):
    return f'Translate from English to Korean: {eng}{kor}'  # 주어진 형식으로 포맷팅

# 데이터셋의 입력과 타겟을 포맷팅하는 함수 정의
def formatting_prompts_func(dataset):
    inputs = dataset['inputs']  # 데이터셋에서 입력 리스트 추출
    targets = dataset['targets']  # 데이터셋에서 타겟 리스트 추출
    return [convert_to_input_text(e, k) for e, k in tqdm(zip(inputs, targets), total=len(inputs))]  # 각 입력과 타겟 쌍에 대해 포맷팅 함수 적용

# 훈련 인자 설정
training_arguments = TrainingArguments(
    output_dir="results",  # 훈련 결과를 저장할 디렉토리
    num_train_epochs=1,  # 훈련 에포크 수
    per_device_train_batch_size=TRAIN_BATCH_SIZE,  # 각 장치에서의 훈련 배치 크기
    gradient_accumulation_steps=GRAD_ACC_STEPS,  # 그래디언트 누적 단계 수
    gradient_checkpointing=USE_GRAD_CHECKPOINTING,  # 그래디언트 체크포인팅 사용 여부
    optim="paged_adamw_32bit",  # 옵티마이저 설정
    save_steps=200,  # 체크포인트 저장 주기(스텝 수)
    logging_steps=10,  # 로깅 주기(스텝 수)
    learning_rate=1e-3,  # 학습률
    weight_decay=0.001,  # 가중치 감쇠
    fp16=False,  # 16비트 부동 소수점 사용 여부
    bf16=True,  # bfloat16 사용 여부
    warmup_ratio=0.05,  # 워밍업 비율
    group_by_length=True,  # 시퀀스 길이에 따라 그룹화 여부
    lr_scheduler_type="constant",  # 학습률 스케줄러 유형
    report_to="none"  # 로깅을 보고할 대상
)

# PEFT(Param Efficient Fine-Tuning) 설정
peft_config = LoraConfig(
    lora_alpha=32,  # LoRA 알파 값
    r=32,  # LoRA 랭크 값
    bias="none",  # 바이어스 설정
    task_type="CAUSAL_LM",  # 태스크 유형
    target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]  # LoRA가 적용될 타겟 모듈들
)

# SFTTrainer 인스턴스 생성
trainer = SFTTrainer(
    model=model,  # 훈련할 모델
    train_dataset=dataset,  # 훈련 데이터셋
    peft_config=peft_config,  # PEFT 설정
    max_seq_length=TRAIN_MAX_SEQ_LENGTH,  # 최대 시퀀스 길이
    tokenizer=tokenizer,  # 토크나이저
    args=training_arguments,  # 훈련 인자
    formatting_func=formatting_prompts_func  # 프롬프트 포맷팅 함수
)

trainer.train()

# Save the model to disk
trainer.model.save_pretrained(save_directory='aya-qlora')
model.config.use_cache = True
model.eval()

# 모델과 LoRA 어댑터 로드
quantization_config = None  # 양자화 설정을 위한 변수를 초기화합니다.
if QUANTIZE_4BIT:  # QUANTIZE_4BIT 플래그가 True일 경우 양자화 설정을 구성합니다.
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 모델을 4비트로 양자화하여 로드합니다.
        bnb_4bit_quant_type="nf4",  # 4비트 양자화 유형을 'nf4'로 설정합니다.
        bnb_4bit_use_double_quant=True,  # 이중 양자화를 사용합니다.
        bnb_4bit_compute_dtype=torch.bfloat16,  # 계산 시 사용할 데이터 타입을 bfloat16으로 설정합니다.
    )

# 사전 학습된 언어 모델을 로드합니다.
loaded_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,  # 사전 학습된 모델의 이름을 지정합니다.
    quantization_config=quantization_config,  # 양자화 설정을 적용합니다 (양자화가 활성화된 경우).
    attn_implementation=attn_implementation,  # 주의 메커니즘(Attention Mechanism) 구현 설정을 적용합니다.
    torch_dtype=torch.bfloat16,  # 모델의 데이터 타입을 bfloat16으로 설정합니다.
    device_map="auto",  # 사용 가능한 모든 GPU를 자동으로 매핑하여 분산 훈련을 설정합니다.
)

# 토크나이저 로드
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# LoRA 어댑터 로드
loaded_model.load_adapter("aya-qlora")

# 테스트용 프롬프트 생성
prompts = [
    'Translate from English to Korean: Rates are competitive, almost always the best in the market',  # 첫 번째 프롬프트 (영어 -> 한국어 번역)
    'Translate from English to Korean: Two far-right Israeli ministers threaten to topple the government if it accepts Biden peace plan'  # 두 번째 프롬프트 (영어 -> 한국어 번역)
]

# 프롬프트를 사용하여 텍스트 생성
generations = generate_aya_23(prompts, loaded_model)

# 각 프롬프트와 생성된 응답을 쌍으로 출력
for p, g in zip(prompts, generations):  # 각 프롬프트와 생성된 텍스트 쌍에 대해 반복
    print(
        "PROMPT", p, "RESPONSE", g, "\n", sep="\n"  # 프롬프트와 응답을 출력. 각 요소는 새로운 줄에 출력됩니다.
    )

#!ls -l /kaggle/working/aya-qlora/*



